## <span style="color:OrangeRed">Abstract</span>
Contrary to the long-held understanding that *deeper* NNs have greater expressivity, it has recently been shown that in over-parameterized models, depth can induce a bias towards simpler representations. In particular, [Masarczyk et al.](http://arxiv.org/abs/2305.19753) observe a definitive separation of the behavior of NN layers: early layers (extractor), which are involved in building complex representations to fit the data, and later layers (tunnel), which compress the learned complex representation from the former into simpler ones. In this work, by adopting an NTK perspective, we provide a theoretical framework to understand this phenomenon. Specifically, we demonstrate the formation of an implicit encoder-decoder structure within an NN, with the encoder/decoder length determined by aspects of the data distribution and the choice of model architecture.
## <span style="color:OrangeRed">Introduction</span>
Deeper layers of neural networks are known to capture more complex representations or structures of the input data. However, a series of works ([Valle-Perez et al.](https://arxiv.org/abs/1805.08522), [Huh et al.](http://arxiv.org/abs/2103.10427), etc.) has recently challenged this view. They show that deep over-parameterized models tend to attain simpler functional forms, a.k.a a *simplicity bias*.  In particular, [Masarczyk et al.](http://arxiv.org/abs/2305.19753)observed that *beyond an optimal depth*, adding hidden layers causes the formation of simpler representations rather than increasingly complex ones. It is to be noted, though, that this simplicity bias is attributed to being a reason behind the poor OOD performance of deep NNs ([Shah et al.](https://proceedings.neurips.cc/paper_files/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf), [Tiwari & Shenoy](http://arxiv.org/abs/2301.13293)). 

To the best of our knowledge, while the individual aspects of increasing model expressivity with depth and simplicity bias in deep over-parameterized models have received significant standalone attention from the community, no unified framework incorporates both. In this work, we attempt to bridge that gap and additionally connect our results to a related observation, *last-layer retraining* ([Kirichenko et al.](https://arxiv.org/abs/2204.02937)). 

Last-layer retraining or, more generally, truncating deeper layers and retraining with linear headers ([Lee et al.](http://arxiv.org/abs/2210.11466), [Evci et al.](http://arxiv.org/abs/2201.03529)) have been suggested as a robustness measure against learning spurious correlations in the data. While all these works have attributed the vulnerability of the later layers to the phenomenon of simplicity bias, and hence the need for their retraining, they do not provide a systematic framework to determine the truncation depth.

In our work, we show that this depth corresponds to the length of the initial implicit encoder layer, the output of which is a coarse-grained (?) representation of the underlying latent features defining the raw inputs themselves.
## <span style="color:OrangeRed">Contributions</span>
## <span style="color:OrangeRed">Setup</span>
## <span style="color:OrangeRed">Theoretical Analysis</span>
## <span style="color:OrangeRed">Experimental Results</span>
## <span style="color:OrangeRed">Discussion</span>
## <span style="color:OrangeRed">Conclusion</span>